{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is equivalent to `demo-word.sh`, `demo-analogy.sh`, `demo-phrases.sh` and `demo-classes.sh` from Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some data, for example: [http://mattmahoney.net/dc/text8.zip](http://mattmahoney.net/dc/text8.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vec\n",
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `word2phrase` to group up similar words \"Los Angeles\" to \"Los_Angeles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /home/bear/bigdata/tox30000.txt\n",
      "Words processed: 1800K     Vocab size: 1162K  \n",
      "Vocab size (unigrams + bigrams): 638379\n",
      "Words in train file: 1890940\n",
      "Words written: 1800K\r"
     ]
    }
   ],
   "source": [
    "word2vec.word2phrase('/home/bear/bigdata/tox30000.txt', '/home/bear/bigdata/tox30000-phrases', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This created a `text8-phrases` file that we can use as a better input for `word2vec`.\n",
    "Note that you could easily skip this previous step and use the text data as input for `word2vec` directly.\n",
    "\n",
    "Now actually train the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /home/bear/bigdata/tox-phrases\n",
      "Vocab size: 135794\n",
      "Words in train file: 15055260\n",
      "Alpha: 0.000002  Progress: 100.05%  Words/thread/sec: 65.48k   lpha: 0.022872  Progress: 8.52%  Words/thread/sec: 63.38k  pha: 0.022856  Progress: 8.59%  Words/thread/sec: 63.44k  ress: 11.62%  Words/thread/sec: 64.45k  2%  Words/thread/sec: 65.35k  : 0.019239  Progress: 23.06%  Words/thread/sec: 65.94k  : 25.29%  Words/thread/sec: 65.99k  0.016340  Progress: 34.65%  Words/thread/sec: 66.04k  ha: 0.015206  Progress: 39.19%  Words/thread/sec: 65.68k  lpha: 0.010148  Progress: 59.42%  Words/thread/sec: 65.09k  09k  hread/sec: 65.33k  sec: 65.33k  .34k  0.006647  Progress: 73.43%  Words/thread/sec: 65.35k  3  Progress: 73.64%  Words/thread/sec: 65.34k  ress: 73.86%  Words/thread/sec: 65.34k  4.07%  Words/thread/sec: 65.32k  Words/thread/sec: 65.31k  hread/sec: 65.31k  ec: 65.30k  29k  006160  Progress: 75.37%  Words/thread/sec: 65.30k  : 77.44%  Words/thread/sec: 65.36k  ds/thread/sec: 65.47k  pha: 0.001551  Progress: 93.81%  Words/thread/sec: 65.51k  3.94%  Words/thread/sec: 65.50k  95.02%  Words/thread/sec: 65.46k  thread/sec: 65.40k  d/sec: 65.43k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2vec('/home/bear/bigdata/tox-phrases', '/home/bear/bigdata/tox_512.bin', size=512, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That created a `text8.bin` file containing the word vectors in a binary format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the clusters of the vectors based on the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /home/bear/bigdata/tox.txt\n",
      "Vocab size: 91435\n",
      "Words in train file: 16869865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.004535  Progress: 81.86%  Words/thread/sec: 160.67k  c: 166.33k  c: 175.81k  51k  2.98%  Words/thread/sec: 177.05k   Progress: 3.20%  Words/thread/sec: 175.29k  %  Words/thread/sec: 177.60k  s/thread/sec: 178.49k  d/sec: 177.59k  179.04k    a: 0.023866  Progress: 4.55%  Words/thread/sec: 179.04k  3818  Progress: 4.74%  Words/thread/sec: 177.71k  rogress: 4.93%  Words/thread/sec: 179.00k  : 5.12%  Words/thread/sec: 179.01k    Words/thread/sec: 179.31k  sec: 179.79k   179.62k  0.023432  Progress: 6.28%  Words/thread/sec: 180.28k  4  Progress: 6.48%  Words/thread/sec: 179.84k  ress: 6.67%  Words/thread/sec: 180.53k  .86%  Words/thread/sec: 179.54k  ords/thread/sec: 180.30k  : 179.53k  ha: 0.023046  Progress: 7.83%  Words/thread/sec: 179.74k  rogress: 8.21%  Words/thread/sec: 180.75k  : 8.40%  Words/thread/sec: 180.22k    Words/thread/sec: 180.51k  sec: 179.87k  k  0.022613  Progress: 9.56%  Words/thread/sec: 180.46k  ess: 9.94%  Words/thread/sec: 179.23k  22421  Progress: 10.33%  Words/thread/sec: 178.79k  lpha: 0.022373  Progress: 10.52%  Words/thread/sec: 178.95k  9.14k  ogress: 11.03%  Words/thread/sec: 178.65k  Progress: 11.21%  Words/thread/sec: 178.71k  gress: 11.46%  Words/thread/sec: 178.69k  2089  Progress: 11.65%  Words/thread/sec: 179.22k  pha: 0.022042  Progress: 11.85%  Words/thread/sec: 179.03k  .17k  /sec: 179.24k  0.021856  Progress: 12.59%  Words/thread/sec: 178.29k  ress: 12.79%  Words/thread/sec: 178.35k  758  Progress: 12.98%  Words/thread/sec: 177.87k  ha: 0.021710  Progress: 13.17%  Words/thread/sec: 177.92k  62k  sec: 177.73k  3%  Words/thread/sec: 178.05k  5  Progress: 14.31%  Words/thread/sec: 177.77k    read/sec: 178.04k  0.021183  Progress: 15.28%  Words/thread/sec: 178.11k  Progress: 15.64%  Words/thread/sec: 178.59k    Progress: 15.96%  Words/thread/sec: 178.60k  78.81k  ad/sec: 179.02k  /sec: 179.07k  ad/sec: 179.06k  20714  Progress: 17.16%  Words/thread/sec: 179.26k  .23k  s/thread/sec: 178.59k  10%  Words/thread/sec: 178.68k  82  Progress: 18.48%  Words/thread/sec: 178.88k  k  hread/sec: 179.05k  0%  Words/thread/sec: 179.30k   Progress: 19.84%  Words/thread/sec: 179.55k   179.61k  179.91k  ords/thread/sec: 179.95k  ogress: 21.19%  Words/thread/sec: 180.32k  sec: 180.15k  .25k  s/thread/sec: 180.27k  s: 22.48%  Words/thread/sec: 180.35k  ress: 22.52%  Words/thread/sec: 180.36k  325  Progress: 22.71%  Words/thread/sec: 180.48k  ha: 0.019277  Progress: 22.90%  Words/thread/sec: 180.59k  ec: 180.53k  %  Words/thread/sec: 180.74k    Progress: 24.04%  Words/thread/sec: 180.85k   ead/sec: 181.11k  Words/thread/sec: 181.21k  64  Progress: 25.36%  Words/thread/sec: 180.99k  018613  Progress: 25.56%  Words/thread/sec: 181.29k  7949  Progress: 28.22%  Words/thread/sec: 181.35k  pha: 0.017901  Progress: 28.41%  Words/thread/sec: 181.42k  /sec: 181.37k  ds/thread/sec: 181.40k  .16%  Words/thread/sec: 180.83k  gress: 29.35%  Words/thread/sec: 180.90k  7617  Progress: 29.55%  Words/thread/sec: 180.68k  pha: 0.017569  Progress: 29.74%  Words/thread/sec: 180.75k  .04k  s/thread/sec: 180.92k  ess: 30.68%  Words/thread/sec: 180.66k  : 0.017237  Progress: 31.06%  Words/thread/sec: 180.57k  : 180.67k  hread/sec: 180.65k    Words/thread/sec: 180.53k  rds/thread/sec: 180.67k    Progress: 32.20%  Words/thread/sec: 180.65k   0.016905  Progress: 32.39%  Words/thread/sec: 180.35k  k  Words/thread/sec: 180.42k   33.34%  Words/thread/sec: 180.52k  Progress: 33.53%  Words/thread/sec: 180.49k  .016573  Progress: 33.72%  Words/thread/sec: 180.60k  180.43k  ords/thread/sec: 180.27k  ogress: 34.86%  Words/thread/sec: 179.93k  16241  Progress: 35.05%  Words/thread/sec: 179.85k  .53k  s/thread/sec: 179.17k  99%  Words/thread/sec: 178.98k  ress: 36.18%  Words/thread/sec: 178.74k  a: 0.015862  Progress: 36.57%  Words/thread/sec: 178.45k  3k  thread/sec: 178.30k  s: 37.51%  Words/thread/sec: 178.08k    177.86k   38.85%  Words/thread/sec: 177.96k  015195  Progress: 39.23%  Words/thread/sec: 177.79k  77.85k  rds/thread/sec: 177.63k  0.19%  Words/thread/sec: 177.43k  /sec: 176.70k  pha: 0.014809  Progress: 40.78%  Words/thread/sec: 176.54k  .63k  /sec: 176.70k  ds/thread/sec: 176.73k  ress: 41.74%  Words/thread/sec: 176.63k    Words/thread/sec: 176.58k  3k  1  Progress: 42.65%  Words/thread/sec: 176.27k  014320  Progress: 42.73%  Words/thread/sec: 176.28k  %  Words/thread/sec: 176.01k  9  Progress: 43.26%  Words/thread/sec: 175.89k  rds/thread/sec: 175.53k  read/sec: 175.40k   44.39%  Words/thread/sec: 175.26k  013809  Progress: 44.78%  Words/thread/sec: 175.13k  75.01k  rds/thread/sec: 174.87k  5.72%  Words/thread/sec: 174.79k  ogress: 45.91%  Words/thread/sec: 174.67k  13477  Progress: 46.11%  Words/thread/sec: 174.60k  lpha: 0.013429  Progress: 46.30%  Words/thread/sec: 174.61k  4.34k  d/sec: 174.26k  rds/thread/sec: 174.19k  7.05%  Words/thread/sec: 173.98k  ogress: 47.24%  Words/thread/sec: 173.84k  13145  Progress: 47.43%  Words/thread/sec: 173.74k  lpha: 0.013097  Progress: 47.63%  Words/thread/sec: 173.61k  /sec: 173.62k  38%  Words/thread/sec: 173.29k  ress: 48.57%  Words/thread/sec: 173.15k  813  Progress: 48.76%  Words/thread/sec: 172.98k  1k  ec: 172.62k  %  Words/thread/sec: 172.29k  ss: 49.90%  Words/thread/sec: 172.09k   0.012433  Progress: 50.28%  Words/thread/sec: 172.00k    : 171.68k  ha: 0.012268  Progress: 50.94%  Words/thread/sec: 171.50k  : 51.23%  Words/thread/sec: 171.32k  .012101  Progress: 51.61%  Words/thread/sec: 171.15k  ad/sec: 170.96k  ords/thread/sec: 170.93k  ogress: 52.75%  Words/thread/sec: 170.59k  ds/thread/sec: 170.51k  .45k  /sec: 170.44k  88%  Words/thread/sec: 170.18k  ress: 54.07%  Words/thread/sec: 170.16k   c: 169.89k  /sec: 169.77k  s: 55.40%  Words/thread/sec: 169.70k  rds/thread/sec: 169.62k  169.40k  ords/thread/sec: 169.15k  ogress: 56.92%  Words/thread/sec: 168.82k  10726  Progress: 57.11%  Words/thread/sec: 168.71k  .32k  168.27k  07%  Words/thread/sec: 168.27k  ogress: 58.41%  Words/thread/sec: 168.18k  a: 0.010341  Progress: 58.65%  Words/thread/sec: 167.99k  c: 167.91k    Words/thread/sec: 167.93k   Progress: 59.79%  Words/thread/sec: 167.86k  167.77k  ords/thread/sec: 167.76k  ogress: 61.13%  Words/thread/sec: 167.47k  pha: 0.009626  Progress: 61.51%  Words/thread/sec: 167.23k  sec: 167.17k  167.05k  8  Progress: 62.66%  Words/thread/sec: 166.77k  : 0.009290  Progress: 62.85%  Words/thread/sec: 166.49k  : 166.01k   Words/thread/sec: 165.48k  : 63.80%  Words/thread/sec: 165.31k  .008956  Progress: 64.19%  Words/thread/sec: 165.24k  : 165.10k  ad/sec: 165.06k  008732  Progress: 65.08%  Words/thread/sec: 164.96k  ogress: 65.32%  Words/thread/sec: 164.77k  : 65.60%  Words/thread/sec: 164.69k  .44k  .41k  46%  Words/thread/sec: 164.34k  92  Progress: 66.84%  Words/thread/sec: 164.32k  k  hread/sec: 164.25k  : 67.98%  Words/thread/sec: 164.32k   Progress: 68.17%  Words/thread/sec: 164.34k  hread/sec: 164.41k    164.40k  read/sec: 164.32k   69.31%  Words/thread/sec: 164.08k  007581  Progress: 69.69%  Words/thread/sec: 163.80k  3.71k  ds/thread/sec: 163.54k  ress: 70.83%  Words/thread/sec: 163.42k  249  Progress: 71.02%  Words/thread/sec: 163.38k  007168  Progress: 71.34%  Words/thread/sec: 163.24k  ec: 163.18k  %  Words/thread/sec: 163.19k    Progress: 72.35%  Words/thread/sec: 163.25k   ead/sec: 162.89k  73.48%  Words/thread/sec: 162.77k  06537  Progress: 73.86%  Words/thread/sec: 162.64k  .59k  /sec: 162.63k  ds/thread/sec: 162.49k  .81%  Words/thread/sec: 162.47k  205  Progress: 75.19%  Words/thread/sec: 162.36k  4k  thread/sec: 161.96k  s: 76.34%  Words/thread/sec: 161.86k    Progress: 76.53%  Words/thread/sec: 161.81k  5793  Progress: 76.84%  Words/thread/sec: 161.70k    Words/thread/sec: 161.26k  rogress: 77.86%  Words/thread/sec: 161.08k  005491  Progress: 78.05%  Words/thread/sec: 161.02k  60.80k  rds/thread/sec: 160.72k  gress: 79.20%  Words/thread/sec: 160.64k  ha: 0.005107  Progress: 79.58%  Words/thread/sec: 160.42k  ec: 160.38k  %  Words/thread/sec: 160.50k  ss: 80.53%  Words/thread/sec: 160.57k   0.004774  Progress: 80.92%  Words/thread/sec: 160.61k   160.69k  Words/thread/sec: 160.70k   81.87%  Words/thread/sec: 160.68k  \r",
      "Alpha: 0.004532  Progress: 81.88%  Words/thread/sec: 160.69k  \r",
      "Alpha: 0.004529  Progress: 81.90%  Words/thread/sec: 160.70k  \r",
      "Alpha: 0.004526  Progress: 81.91%  Words/thread/sec: 160.70k  \r",
      "Alpha: 0.004523  Progress: 81.92%  Words/thread/sec: 160.72k  \r",
      "Alpha: 0.004520  Progress: 81.93%  Words/thread/sec: 160.74k  \r",
      "Alpha: 0.004517  Progress: 81.94%  Words/thread/sec: 160.74k  \r",
      "Alpha: 0.004514  Progress: 81.96%  Words/thread/sec: 160.75k  \r",
      "Alpha: 0.004511  Progress: 81.97%  Words/thread/sec: 160.76k  \r",
      "Alpha: 0.004508  Progress: 81.98%  Words/thread/sec: 160.74k  \r",
      "Alpha: 0.004505  Progress: 81.99%  Words/thread/sec: 160.73k  \r",
      "Alpha: 0.004502  Progress: 82.00%  Words/thread/sec: 160.71k  \r",
      "Alpha: 0.004499  Progress: 82.02%  Words/thread/sec: 160.71k  \r",
      "Alpha: 0.004496  Progress: 82.03%  Words/thread/sec: 160.72k  \r",
      "Alpha: 0.004493  Progress: 82.04%  Words/thread/sec: 160.74k  \r",
      "Alpha: 0.004490  Progress: 82.05%  Words/thread/sec: 160.73k  \r",
      "Alpha: 0.004487  Progress: 82.06%  Words/thread/sec: 160.74k  \r",
      "Alpha: 0.004484  Progress: 82.08%  Words/thread/sec: 160.75k  \r",
      "Alpha: 0.004481  Progress: 82.09%  Words/thread/sec: 160.75k  \r",
      "Alpha: 0.004478  Progress: 82.10%  Words/thread/sec: 160.77k  \r",
      "Alpha: 0.004475  Progress: 82.11%  Words/thread/sec: 160.78k  \r",
      "Alpha: 0.004472  Progress: 82.12%  Words/thread/sec: 160.74k  \r",
      "Alpha: 0.004469  Progress: 82.14%  Words/thread/sec: 160.76k  \r",
      "Alpha: 0.004466  Progress: 82.15%  Words/thread/sec: 160.75k  \r",
      "Alpha: 0.004463  Progress: 82.16%  Words/thread/sec: 160.76k  \r",
      "Alpha: 0.004460  Progress: 82.17%  Words/thread/sec: 160.77k  \r",
      "Alpha: 0.004457  Progress: 82.18%  Words/thread/sec: 160.79k  \r",
      "Alpha: 0.004454  Progress: 82.20%  Words/thread/sec: 160.78k  \r",
      "Alpha: 0.004451  Progress: 82.21%  Words/thread/sec: 160.76k  \r",
      "Alpha: 0.004448  Progress: 82.22%  Words/thread/sec: 160.78k  \r",
      "Alpha: 0.004445  Progress: 82.23%  Words/thread/sec: 160.77k  \r",
      "Alpha: 0.004442  Progress: 82.24%  Words/thread/sec: 160.77k  \r",
      "Alpha: 0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.000002  Progress: 100.03%  Words/thread/sec: 164.55k  0.81k  .20%  Words/thread/sec: 160.87k  gress: 83.39%  Words/thread/sec: 160.91k  4106  Progress: 83.59%  Words/thread/sec: 160.92k  05k  sec: 161.03k  s/thread/sec: 161.05k  ess: 84.73%  Words/thread/sec: 161.14k  72  Progress: 84.92%  Words/thread/sec: 161.14k  a: 0.003724  Progress: 85.11%  Words/thread/sec: 161.17k  c: 161.21k    Words/thread/sec: 161.16k  ss: 86.23%  Words/thread/sec: 161.22k  0.003392  Progress: 86.44%  Words/thread/sec: 161.34k  161.37k  ords/thread/sec: 161.43k  thread/sec: 161.54k  03061  Progress: 87.77%  Words/thread/sec: 161.59k  lpha: 0.003013  Progress: 87.96%  Words/thread/sec: 161.60k  /sec: 161.72k  ds/thread/sec: 161.79k  ress: 88.91%  Words/thread/sec: 161.87k  a: 0.002681  Progress: 89.29%  Words/thread/sec: 162.04k  c: 162.15k    Words/thread/sec: 162.24k   Progress: 90.43%  Words/thread/sec: 162.33k  ad/sec: 162.48k  1.56%  Words/thread/sec: 162.57k  2017  Progress: 91.94%  Words/thread/sec: 162.63k  79k  /thread/sec: 162.82k  ss: 93.08%  Words/thread/sec: 162.94k   0.001637  Progress: 93.46%  Words/thread/sec: 163.02k   163.16k  Words/thread/sec: 163.30k  rogress: 94.60%  Words/thread/sec: 163.34k  lpha: 0.001256  Progress: 94.99%  Words/thread/sec: 163.45k  /sec: 163.54k  74%  Words/thread/sec: 163.62k  73  Progress: 96.12%  Words/thread/sec: 163.68k  k   Words/thread/sec: 163.94k  Progress: 97.45%  Words/thread/sec: 164.01k  Words/thread/sec: 164.19k  : 0.000205  Progress: 99.19%  Words/thread/sec: 164.40k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2clusters('/home/bear/bigdata/tox.txt', '/home/bear/bigdata/tox-clusters.txt', 300, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That created a `text8-clusters.txt` with the cluster for every word in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `word2vec` binary file created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.load('/home/bear/bigdata/tox.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the vocabulary as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['</s>', 'exposure', 'effects', ..., 'r665', 'oobiphenol',\n",
       "       'recipavrin'], dtype='<U78')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or take a look at the whole matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135794, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08015626,  0.08850129, -0.07670335, ..., -0.02626957,\n",
       "        -0.03316621,  0.0614953 ],\n",
       "       [-0.06314697, -0.0020846 ,  0.01790366, ...,  0.06257509,\n",
       "        -0.05106722,  0.01221308],\n",
       "       [-0.00866614, -0.00828651,  0.03921769, ...,  0.04933339,\n",
       "         0.0625715 ,  0.03236538],\n",
       "       ...,\n",
       "       [-0.01647791, -0.12637718, -0.05695276, ...,  0.05563355,\n",
       "         0.05544947,  0.01216888],\n",
       "       [-0.08948104, -0.10090277,  0.00732808, ..., -0.03293901,\n",
       "         0.00705413, -0.07581794],\n",
       "       [ 0.00969618, -0.09953561,  0.05869497, ...,  0.09755769,\n",
       "         0.01010175, -0.09685887]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retreive the vector of individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['liver'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00105867, -0.07775147, -0.05253823, -0.00679678,  0.03260306,\n",
       "        0.03492132,  0.04879236,  0.06382352,  0.09693788, -0.10532206])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['liver'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the distance between two or more (all combinations) words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('liver', 'heart', 0.46211296256692835),\n",
       " ('liver', 'cell', 0.16932151990970476),\n",
       " ('heart', 'cell', 0.01176196145551267)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distance(\"liver\", \"heart\", \"cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity\n",
    "\n",
    "We can do simple queries to retreive words similar to \"socks\" based on cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  175,  1008,  2797,   206,  1069,  1706,  3362,  3234, 10341,\n",
       "          678]),\n",
       " array([0.7051077 , 0.66506537, 0.57917927, 0.57059308, 0.51442374,\n",
       "        0.48549201, 0.48429055, 0.47507805, 0.46735535, 0.46211296]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = model.similar(\"liver\")\n",
    "indexes, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returned a tuple with 2 items:\n",
    "1. numpy array with the indexes of the similar words in the vocabulary\n",
    "2. numpy array with cosine similarity to each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the words for those indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hepatic', 'livers', 'hepatocellular', 'kidney', 'organ', 'testis',\n",
       "       'pancreatic', 'pancreas', 'liver_homogenate', 'heart'],\n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a helper function to create a combined response as a numpy [record array](http://docs.scipy.org/doc/numpy/user/basics.rec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.array([('hepatic', 0.7051077 ), ('livers', 0.66506537),\n",
       "           ('hepatocellular', 0.57917927), ('kidney', 0.57059308),\n",
       "           ('organ', 0.51442374), ('testis', 0.48549201),\n",
       "           ('pancreatic', 0.48429055), ('pancreas', 0.47507805),\n",
       "           ('liver_homogenate', 0.46735535), ('heart', 0.46211296)],\n",
       "          dtype=[('word', '<U78'), ('metric', '<f8')])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_response(indexes, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is easy to make that numpy array a pure python response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hepatic', 0.7051076955702468),\n",
       " ('livers', 0.6650653680083782),\n",
       " ('hepatocellular', 0.5791792659172474),\n",
       " ('kidney', 0.570593078628412),\n",
       " ('organ', 0.5144237445663715),\n",
       " ('testis', 0.48549201288328525),\n",
       " ('pancreatic', 0.4842905491948804),\n",
       " ('pancreas', 0.4750780490688806),\n",
       " ('liver_homogenate', 0.46735534591553507),\n",
       " ('heart', 0.46211296256692835)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we trained the model with the output of `word2phrase` we can ask for similarity of \"phrases\", basically compained words such as \"Los Angeles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cell_lines', 0.8452627475811616),\n",
       " ('hepg2', 0.7789955019696182),\n",
       " ('cellline', 0.7654203295789872),\n",
       " ('fibroblasts', 0.7449751605717496),\n",
       " ('lung_fibroblasts', 0.7369620083890245),\n",
       " ('fibroblast', 0.7307756186508869),\n",
       " ('hela_cells', 0.7303405837250463),\n",
       " ('a549', 0.7097297177166331),\n",
       " ('cells', 0.7053972818334586),\n",
       " ('immortalized', 0.7014494405101693)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = model.similar('cell_line')\n",
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its possible to do more complex queries like analogies such as: `king - man + woman = queen` \n",
    "This method returns the same as `cosine` the indexes of the words in the vocab and the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 947,  175,  412,  843, 1008, 1645, 1655, 1461, 2117,  370]),\n",
       " array([0.21516253, 0.20577065, 0.16443839, 0.1630304 , 0.16087712,\n",
       "        0.15813997, 0.1553662 , 0.15516005, 0.1545503 , 0.15339436]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = model.analogy(pos=[\"breast_cancer\", \"liver\"], neg=['breast'])\n",
    "indexes, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hepatotoxicity', 0.21516252542431225, 281),\n",
       " ('hepatic', 0.20577064842314757, 123),\n",
       " ('hepatocytes', 0.16443839324541545, 13),\n",
       " ('liver_microsomes', 0.16303039670439448, 119),\n",
       " ('livers', 0.1608771203520329, 108),\n",
       " ('hepatocyte', 0.15813997420322706, 67),\n",
       " ('cyp2e1', 0.1553661984648455, 137),\n",
       " ('ccl4', 0.15516004641554593, 188),\n",
       " ('apap', 0.15455030257951768, 246),\n",
       " ('oxidative', 0.15339436227934383, 55)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = word2vec.load_clusters('/home/bear/bigdata/tox-clusters.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see get the cluster number for individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['</s>', 'exposure', 'cells', ..., 'r665', 'oobiphenol',\n",
       "       'recipavrin'], dtype='<U66')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see get all the words grouped on an specific cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.get_words_on_cluster(90).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['phenobarbital', 'inducer', 'dexamethasone', 'dosedependently',\n",
       "       'ac', '3methylcholanthrene', 'tpa', '3mc', 'ketoconazole', 'bnf'],\n",
       "      dtype='<U66')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.get_words_on_cluster(90)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the clusters to the word2vec model and generate a response that includes the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clusters = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes, metrics = model.analogy(pos=[\"breast_cancer\", \"liver\"], neg=[\"breast\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hepatotoxicity', 0.21516252542431225, 281),\n",
       " ('hepatic', 0.20577064842314757, 123),\n",
       " ('hepatocytes', 0.16443839324541545, 13),\n",
       " ('liver_microsomes', 0.16303039670439448, 119),\n",
       " ('livers', 0.1608771203520329, 108),\n",
       " ('hepatocyte', 0.15813997420322706, 67),\n",
       " ('cyp2e1', 0.1553661984648455, 137),\n",
       " ('ccl4', 0.15516004641554593, 188),\n",
       " ('apap', 0.15455030257951768, 246),\n",
       " ('oxidative', 0.15339436227934383, 55)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this review, we discuss the genetic factors that underlie variability to drug response and known pharmacogenomic associations and how these differ between populations, with an emphasis on the current knowledge in cardiovascular pharmacogenomics'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Race and ancestry have long been associated with differential risk and outcomes to disease as well as responses to medications. These differences in drug response are multifactorial with some portion associated with genomic variation. The field of pharmacogenomics aims to predict drug response in patients prior to medication administration and to uncover the biological underpinnings of drug response. The field of human genetics has long recognized that genetic variation differs in frequency between ancestral populations, with some single nucleotide polymorphisms found solely in one population. Thus far, most pharmacogenomic studies have focused on individuals of European and East Asian ancestry, resulting in a substantial disparity in the clinical utility of genetic prediction for drug response in US minority populations. In this review, we discuss the genetic factors that underlie variability to drug response and known pharmacogenomic associations and how these differ between populations, with an emphasis on the current knowledge in cardiovascular pharmacogenomics\"\n",
    "summarize(text, words=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
